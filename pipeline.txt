# standardize data after polynomial basis expansion?
# function to generate test predictions?..... is this run.py descritta sul doc?
# implement confusion matrix....
# remove features 15, 18, 20
# balance dataset done
# remove 30% of 40% dataset containing nans and misses (to balance dataset!) yep done
# test newton's method, random forest?... idk do we have time?

- shuffle data!

- normalize data = mean to 0 and variance to 1

- -999 -> mean or median

- Different approaches to dealing with -999 values: mean or median or eliminate features

- Which features are always corrupted? three groups, 15 40 70

- Eliminate linearly dependent rows/ columns: no directly linearly dependent features. Role of linear dependency on rows
unclear, ASK TA

- checking for linear dependence; before or after basis expansion? does it make any difference? ASK TA

- Relationships between features : covariance matrix = eliminate features with correlation over 90%?

- Distributions of the variables : explored using histograms and graphs of the hit/miss ratio for each variable

- Polynomial expansion done

- Compare different model types

- which loss functions to use??